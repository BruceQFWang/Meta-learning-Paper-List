# Meta learning/Learning to learn/AutoML

### Top
Suggested books by Mike Jordan at Berkeley:[link](https://news.ycombinator.com/item?id=1055389)

## Papers

### 2022
- <a name="todo"></a> On the Importance of Firth Bias Reduction in Few-Shot Classification (**ICLR2022**) [[paper](https://openreview.net/pdf?id=DNRADop4ksB)]
- <a name="todo"></a> Continuous-Time Meta-Learning with Forward Mode Differentiation (**ICLR2022**) [[paper](https://openreview.net/pdf?id=57PipS27Km)]
- <a name="todo"></a> Finetuned Language Models are Zero-Shot Learners (**ICLR2022**) [[paper](https://openreview.net/pdf?id=gEZrGCozdqR)]
- <a name="todo"></a> How to Train Your MAML to Excel in Few-Shot Classification (**ICLR2022**) [[paper](https://openreview.net/pdf?id=49h_IkpJtaE)]
- <a name="todo"></a> Task Affinity with Maximum Bipartite Matching in Few-Shot Learning (**ICLR2022**) [[paper](https://openreview.net/pdf?id=u2GZOiUTbt)]
- <a name="todo"></a> Bootstrapped Meta-Learning (**ICLR2022**) [[paper](https://openreview.net/pdf?id=b-ny3x071E5)]
- <a name="todo"></a> Meta-Learning with Fewer Tasks through Task Interpolation (**ICLR2022**) [[paper](https://openreview.net/pdf?id=ajXWF7bVR8d)]
- <a name="todo"></a> Vision-Based Manipulators Need to Also See from Their Hands (**ICLR2022**) [[paper](https://openreview.net/pdf?id=RJkAHKp7kNZ)]
- <a name="todo"></a> Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners (**ICLR2022**) [[paper](https://openreview.net/pdf?id=u2GZOiUTbt)]
- <a name="todo"></a> Subspace Regularizers for Few-Shot Class Incremental Learning (**ICLR2022**) [[paper](https://openreview.net/pdf?id=boJy41J-tnQ)]
- <a name="todo"></a> ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning (**ICLR2022**) [[paper](https://openreview.net/pdf?id=zRJu6mU2BaE)]
- <a name="todo"></a> Temporal Alignment Prediction for Supervised Representation Learning and Few-Shot Sequence Classification (**ICLR2022**) [[paper](https://openreview.net/pdf?id=p3DKPQ7uaAi)]
- <a name="todo"></a>  LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5 (**ICLR2022**) [[paper](https://openreview.net/pdf?id=HCRVf71PMF)]
- <a name="todo"></a>  Few-shot Learning via Dirichlet Tessellation Ensemble (**ICLR2022**) [[paper](https://openreview.net/pdf?id=6kCiVaoQdx9)]
- <a name="todo"></a>  Training Data Generating Networks: Shape Reconstruction via Bi-level Optimization (**ICLR2022**) [[paper](https://openreview.net/pdf?id=dDo8druYppX)]
- <a name="todo"></a>  Switch to Generalize: Domain-Switch Learning for Cross-Domain Few-Shot Classification (**ICLR2022**) [[paper](https://openreview.net/pdf?id=H-iABMvzIc)]
- <a name="todo"></a>  On the Role of Neural Collapse in Transfer Learning (**ICLR2022**) [[paper](https://openreview.net/pdf?id=SwIp410B6aQ)]
- <a name="todo"></a>  Generalizing Few-Shot NAS with Gradient Matching (**ICLR2022**) [[paper](https://openreview.net/pdf?id=_jMtny3sMKU)]




### 2019.3.19
**[1]** Andrychowicz, Marcin, et al. **Learning to learn by gradient descent by gradient descent.** Advances in Neural Information Processing Systems. 2016. [link](https://arxiv.org/pdf/1606.04474.pdf)   (使用LSTM学习梯度)


**[2]** Finn, Chelsea, Pieter Abbeel, and Sergey Levine.**Model-agnostic meta-learning for fast adaptation of deep networks.** Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.[link](https://arxiv.org/pdf/1703.03400.pdf) (二次梯度)

**[3]** Snell, Jake, Kevin Swersky, and Richard Zemel. **Prototypical networks for few-shot learning.** Advances in Neural Information Processing Systems. 2017.[link](http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf) (四种度量网络的一个)

**[4]** Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. **Human-level concept learning through probabilistic program induction.** Science 350.6266 (2015): 1332-1338.[link](https://www.sas.upenn.edu/~astocker/lab/teaching-files/PSYC739-2016/Lake_etal2015.pdf)  (偏向于特征工程学习先验)

### 2019.3.25
**[5]** Sung, Flood, et al. **Learning to compare: Relation network for few-shot learning.** Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.[link](http://openaccess.thecvf.com/content_cvpr_2018/papers/Sung_Learning_to_Compare_CVPR_2018_paper.pdf) (四种度量网络的一个)

**[6]** Vinyals, Oriol, et al. **Matching networks for one shot learning.** Advances in neural information processing systems. 2016.[link](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)  (四种度量网络的一个)

**[7]** Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. **Siamese neural networks for one-shot image recognition.** ICML Deep Learning Workshop. Vol. 2. 2015.[link](http://www.cs.toronto.edu/~gkoch/files/msc-thesis.pdf) (孪生网络)

### 2019.3.27
**[8]** Ravi, Sachin, and Hugo Larochelle. **Optimization as a model for few-shot learning.** (2016).[link](https://openreview.net/pdf?id=rJY0-Kcll) 

### 2019.3.28
**[9]** Chen, Yutian, et al. **Learning to learn without gradient descent by gradient descent.** Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.[link](https://arxiv.org/pdf/1611.03824.pdf)  (梯度优化黑盒函数)

### 2019.3.31
**[10]** Xu, Ju, and Zhanxing Zhu. **Reinforced continual learning.** Advances in Neural Information Processing Systems. 2018.[link](https://papers.nips.cc/paper/7369-reinforced-continual-learning.pdf) 

### 2019.4.1
**[11]** Pham, Hieu, et al. **Efficient neural architecture search via parameter sharing.** arXiv preprint arXiv:1802.03268 (2018).[link](https://arxiv.org/pdf/1802.03268.pdf?fbclid=IwAR1RHoGyzFPepWpSyNA1TcySIjEto2scD7Fg3Pk6KOUygRNKXiA_r68MIkI)  

### 2019.4.2
**[12]** Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. **Neural architecture search: A survey.** arXiv preprint arXiv:1808.05377 (2018).[link](https://arxiv.org/pdf/1808.05377.pdf) (很好的一篇NAS的综述)

### 2019.4.3
**[13]** Quanming, Yao, et al. **Taking human out of learning applications: A survey on automated machine learning.** arXiv preprint arXiv:1810.13306 (2018).[link](https://arxiv.org/pdf/1810.13306.pdf  (一篇不错的AutoML的综述)

### 2019.4.9
**[14]** Nichol, Alex, Joshua Achiam, and John Schulman. **On first-order meta-learning algorithms.** arXiv preprint arXiv:1803.02999 (2018).[link](https://arxiv.org/pdf/1803.02999.pdf)  (两篇分析并拓展MAML算法的文章，reptile版本)

**[15]** Antoniou, Antreas, Harrison Edwards, and Amos Storkey. **How to train your MAML.** arXiv preprint arXiv:1810.09502 (2018).[link](https://arxiv.org/pdf/1810.09502.pdf)  (两篇分析并拓展MAML算法的文章)

### 2019.4.17
**[16]** Frank Hutter, Lars Kotthoff, Joaquin Vanschoren**Automatic Machine Learning:Methods, Systems, Challenges** .[link](https://www.automl.org/book/) (automl介绍全面的书)

### 2019.4.18
**[17]** Dong, Jianfeng, et al. **Dual Encoding for Zero-Example Video Retrieval.** CVPR, 2019. [link](http://lixirong.net/pub/cvpr2019-dense-encoding.pdf) (孪生网络在零样本学习的应用，视频检索)

### 2019.4.28
**[18]** Frans K, Ho J, Chen X, et al. **Meta learning shared hierarchies[J].** arXiv preprint arXiv:1710.09767, 2017. [link](https://arxiv.org/pdf/1710.09767.pdf)  (openai天才高中生)

### 2019.5.1
**[19]** Wang, Duo, et al. **A Hybrid Approach with Optimization-Based and Metric-Based Meta-Learner for Few-Shot Learning.** Neurocomputing (2019).[link](https://arxiv.org/pdf/1904.03014.pdf) (MAML训练feature extractor,meta classifier做分类)

### 2019.5.6
**[20]**  Yingtian Zou , Jiashi Feng. **Hierarchical Meta Learning.** arXiv preprint arXiv:1904.09081v1, 2019. [link](https://arxiv.org/pdf/1904.09081v1.pdf)  (在MAML的基础上考虑不相似的domain)

### 2019.5.10
**[20]**  Mikhail Khodak, Maria-Florina Balcan, Ameet Talwalkar. **Provable Guarantees for Gradient-Based Meta-Learning.** arXiv preprint arXiv:1902.10644, 2019. [link](https://arxiv.org/pdf/1902.10644.pdf)  (ICML2019)


### 2019.5.11
**[21]**  **Meta-Learning: Learning to Learn Fast**[link](https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html) (一个简单介绍元学习STOA方法的博客，主要基于图像分类)

**[22]** Joaquin Vanschoren. **Meta-Learning: A Survey.** arXiv preprint arXiv:1810.03548, 2018.[link](https://arxiv.org/pdf/1810.03548.pdf)

### 2019.7.27
**[23]** NIPS2018 accepted meta-learning paper [link](http://metalearning.ml/2018/)
